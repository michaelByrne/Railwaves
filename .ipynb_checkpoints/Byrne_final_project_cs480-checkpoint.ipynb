{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trackside Acoustic Rail Traffic Classification with Neural Networks and Linear Discriminant Analysis\n",
    "\n",
    "#### Michael Byrne, michael.byrne@colostate.edu\n",
    "\n",
    "\n",
    "### Abstract\n",
    "_Research into acoustic event detection is thus far largely focused on idealized scenarios involving highly differentiated classes. Industry-geared efforts are, meanwhile, almost entirely focused on speech processing. Here we attempt to advance practical acoustic classification into the domain of industrial surveillance, particularly the classification of rail traffic in real-time. With audio data collected over several weeks in Vancouver, WA, we employ feature subsets extending Mel-Frequency Cepstral Coefficients (MFCCs) for classification tasks that are unusually human ear agnostic. Building on limited earlier work in rail traffic classification, we develop and test both linear logistic regression and neural network models. A low-power embedded device for perorming real-time classification is then demonstrated. Results suggest that industrial acoustic surveillance is a currently viable technology that nonetheless remains underutilized._\n",
    "\n",
    "## 1. Introduction\n",
    "Virtually every research effort into acoustic classification using machine learning assumes an intermediate human ear. Classifiers are built to simulate its physiology and, crucially, its limitations. Given tasks such as scene classification and speech recognition, this is reasonable. The nature of the classified in these cases is bound by human semantics. But it's trivial to imagine scenarios in which making this assumption is no longer reasonable. Take sounds from the natural world, for example, or industrial soundscapes. In the machine learning context, we can imagine the human ear being a limitation rather than a neccessary interpretive layer. \n",
    "\n",
    "Here we consider a scenario in which a human interpreter would almost certainly be unable to make classifications based only on acoustic input. To most anyone, a train sounds like a train. It's loud and noisy. But we know from preliminary spectral analyses and indeed some careful listening on the part of well-trained human ears that marked differences exist in the frequency distributions of train sounds that persist at variable distances and among variable acoustic backgrounds. We hypothesize that different varieties of rail car offer acoustic signatures for the simple reason that rail cars are mechanically distinct in several key ways: aerodynamic profile, weight distribution, loose hardware (e.g. boxcar doors, gondola hatches, walkways, chains, etc), shock absorption, typical cargo weight. \n",
    "\n",
    "We hypothesize further that it's possible to craft acoustic models of \"normal\" car operation. Such models have implications for not just railway operation&mdash;such as improved trackside anomaly detection&mdash;but industrial operations in general. One might imagine an acoustic event detection system replacing or supplanting human visual inspections in dangerous or inaccessible environments, or where frequent human inspections are impractical or expensive. Moreover, by implementing a model of normal operation, it may be possible to \\textit{predict} mechanical failures that have so-far been unobserved. This is roughly analogous to machine learning-based anomaly detection currently being studied in computer security[1]. \n",
    "\n",
    "Our work was initially motivated by railcar counting efforts in the Pacific-Northwest[2]. Recent incidents involving derailed cars carrying petroleum products have caused a spike in concern within trackside communities. Nonetheless, major railways are resistant to releasing data on rail traffic. This is a source of great contention and has led to informal oil car counting efforts along rail right-of-ways in Washington state and Oregon. These are incomplete and sporadic&mdash;hardly surprising given the relative low frequency of rail traffic even along major corridors and the corresponding time commitment. \n",
    "\n",
    "We imagine a small low-power embedded device to automate this task. It will retrieve pre-trained models from the cloud as they are refined and updated and will use those models to perform classifications autonomously. The classifications will then be uploaded back to the cloud. In the ideal case, the Railbox device would need only to have its batteries changed with no other human intervention required. \n",
    "\n",
    "In this report, we describe the collection of over 40,000 points of acoustic data corresponding to rail traffic in and around Portland, Oregon. We use a subset of this data to train both logistic regression and neural network models and we explore the parameter space of both approaches. We create models based on features corresponding to both wide Fast Fourier Transform (FFT) bins and extracted spectral constrast features, which are commonly used in music analysis algorithms and in enhancing speech intelligibility in noisy environments. We then describe a prototype system for performing autonomous real-time classification with a low-cost microcontroller and electret microphone. \n",
    "\n",
    "\n",
    "## 2. Background\n",
    "\n",
    "Virtually all acoustic classification research to date has focused on speech processing. Given market enthusiasm, that's to be expected. A smaller subset of the field involves acoustic event detection, wherein discrete phenomena are modeled using specialized machine learning approaches suitable for time series data. Naturally, events have beginnings and endings. They evolve through time where individual data points necessarily encode information about earlier data points. The time series relationships built into acoustic event detection problems make them well-suited for Hidden Markov Models (HMMs). In an HMM, data points correspond to sequential observations wherein a given observation can be considered as the product of the next earliest observation in the sequence. Like this, observations may be chained together such that a joint probability can be calculated for a sequence of observations. \n",
    "\n",
    "Examples of acoustic event detection outside of speech processing include classification of urban noises (dogs barking, car horns, etc)[3], whale calls[4], frog calls[5], and bird calls[6]. Much of this work is guided by the ongoing IEEE challenge on Detection and Classification of Acoustic Scenes and Events and its associated datasets and open-source baseline event detection framework.\n",
    "\n",
    "Nearly every example of acoustic event detection involves time series data. What we consider here, however, is acoustic data that consists of independent observations. The train noise we are interested in evaluating does not change through time, or does not change through time in a meaningful way. This enables us to use more conventional machine learning techniques based on models built from independent data points or data points that can be considered to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection\n",
    "Trains were recorded using the built-in microphone of an iPhone SE at a distance of 10 to 15 feet. Recording took place over several nights at various points on the platforms of the Vancouver, WA Amtrak station. As it's the confluence of several major rail lines, this corridor sees heavy traffic, averaging two or more trains per hour. Oil cars recorded were either part of mixed-freight consists or unit trains (trains pulling only one variety of car for usually one customer). Intermodal trains are by default unit trains. Train speeds were between 15 and 25 mph. Initial recordings were between 30 and 90 seconds and were chunked into one-second clips using an external Python script.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Set 1: Naive FFT\n",
    "Because we're seeking to classify based on audio features whose interpretation is not neccessarily tied to human hearing, we hypothesize that a naive approach using broad spectral bands as features will yield a suitable predictive model. \n",
    "\n",
    "Plotting waveforms from two railcar types reveals little beyond the limited observation that oil cars have a denser signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/CS/cs480/railbox/final_project_byrne/samples/oil_test_0.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22bc9eb883a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mXoil\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr_oil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples/oil_test_0.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mTime_oil\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXoil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msr_oil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXoil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrawread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrawread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawAudioFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/CS/cs480/railbox/final_project_byrne/samples/oil_test_0.wav'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "Xoil,sr_oil = librosa.load(\"samples/oil_test_0.wav\")\n",
    "Time_oil=np.linspace(0, len(Xoil)/sr_oil, num=len(Xoil))\n",
    "plt.figure(1)\n",
    "plt.title('Sample Oil Train Wav...')\n",
    "plt.plot(Time_oil,Xoil)\n",
    "plt.show()\n",
    "\n",
    "Xstack,sr_stack = librosa.load(\"samples/0418a6.wav\")\n",
    "Time_stack=np.linspace(0, len(Xstack)/sr_stack, num=len(Xstack))\n",
    "plt.figure(1)\n",
    "plt.title('Sample Intermodal Train Wav...')\n",
    "plt.plot(Time_stack,Xstack)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now spectrograms of the same two signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import specgram\n",
    "specgram(np.array(Xoil), Fs=sr_oil)\n",
    "plt.title('Sample Oil Train Spec...')\n",
    "plt.show()\n",
    "plt.title('Sample Intermodal Train Spec...')\n",
    "specgram(np.array(Xstack), Fs=sr_stack)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectrograms aren't dramatically different, but we can see some bands clustered around 2000 Hz. Intuitively, this aligns well with the respective sample waveforms. Later we will reconsider this observation in light of the weights generated during neural network and LDA training.  \n",
    "\n",
    "To process audio recordings for analysis, we take one second samples (chunked from 1 - 3 minute field recordings using an external Python script) and further subdivide them into 50 ms frames. Using non-time series data allows us to reduce to such small frame sizes because we can consider our samples as indepedent observations; that is, the ordering of our frames is meaningless with respect to the classification task. In classifying discrete acoustic events, such as car horns or animal calls, clearly we would have abide by a lower limit given by the length of the event. Our frame size is somewhat arbitrarily chosen to account for between-car gaps and to ensure a fully \"saturated\" acoustic profile. \n",
    "\n",
    "Below is only an example of the processing involved in a simple audio recording. File names are listed in a subdirectory along with labels corresponding to the type of railcar. Every car type that is not an oil car is converted to class 0, while oil cars are converted to class 1. For simplicity, in our experiments we consider only oil cars and \"double-stack\" intermodal cars. Of all railcar types, these two varieties have the most dramatic difference in aerodynamic profile, and, anecdotally, they have exceptionally characteristic sounds.  \n",
    "\n",
    "Because the preprocessing involved is fairly time-intensive, we have included only commented-out example code in the following cell. It takes as input a csv file containing a list of wav files that have been labeled by car type. Every one-minute file is divided into 50 frames. Each frame undergoes a Fast Fourier Transform (FFT) and the resulting frequency-domain data is consolidated into 10 bins per frame. Frequency data for each bin is then normalized and written to file along with the same label that accompanied the original wav file. Data files are then consolidated into one master data file that will later become training data. \n",
    "\n",
    "A sampling of data looks like this: \n",
    "\n",
    "<blockquote>0.719,0.513,0.376,0.191,0.165,0.102,0.047,0.035,0.027,1  \n",
    "0.793,0.494,0.268,0.162,0.148,0.079,0.034,0.022,0.016,1  \n",
    "0.717,0.563,0.327,0.198,0.124,0.073,0.031,0.026,0.012,1  \n",
    "0.783,0.485,0.305,0.175,0.137,0.081,0.029,0.021,0.013,1  \n",
    "0.684,0.541,0.398,0.213,0.16,0.09,0.037,0.025,0.014,1  \n",
    "0.72,0.541,0.313,0.218,0.179,0.086,0.041,0.038,0.014,1  \n",
    "0.683,0.563,0.37,0.212,0.159,0.081,0.04,0.029,0.01,1  \n",
    "0.726,0.487,0.352,0.236,0.194,0.126,0.04,0.03,0.015,1  \n",
    "0.713,0.501,0.404,0.185,0.177,0.094,0.045,0.027,0.014,1\n",
    "0.807,0.469,0.311,0.124,0.104,0.068,0.045,0.016,0.012,0</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('nbagg')\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.fftpack import fft, dct\n",
    "# import scipy.io.wavfile\n",
    "# import numpy\n",
    "# from IPython import display\n",
    "# import time\n",
    "# import csv\n",
    "# %matplotlib inline\n",
    "\n",
    "### Here we read in file names and labels for audio samples\n",
    "# with open('samples.csv') as f:\n",
    "#     samples = []\n",
    "#     for line in f:\n",
    "#         sample = line.split()\n",
    "#         samples.append(sample)\n",
    "\n",
    "### We open each sample and apply a normalization to emphasize higher frequencies\n",
    "### We then divide each sample into smaller frames and apply the FFT\n",
    "### The frequency domain data is then consolidated into 10 bins and normalized from 0 - 1\n",
    "# for rec in samples:\n",
    "#     bins = []\n",
    "#     sample_rate, signal = scipy.io.wavfile.read(\"samples/\" + rec[0])\n",
    "#     plt.plot(signal)\n",
    "#     pre_emphasis = 0.97\n",
    "#     es = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "#     frames = []\n",
    "#     num_samples = es.size\n",
    "#     frame_size = num_samples / 50\n",
    "#     upper =  0\n",
    "#     lower = 0\n",
    "#     for i in range(50):\n",
    "#         lower = i * frame_size\n",
    "#         upper = lower + frame_size - 1\n",
    "#         frames.append(es[lower:upper])\n",
    "#     for frame in frames:\n",
    "#         c = fft(frame) # calculate fourier transform (complex numbers list)\n",
    "#         d = len(c)/2  # you only need half of the fft list (real signal symmetry)\n",
    "#         er = abs(c[:(d-1)])\n",
    "#         # plt.plot(er,'r') \n",
    "#         # plt.show()\n",
    "#         bins = []\n",
    "#         bin_size = d / 10\n",
    "#         for i in range(0,9):\n",
    "#             lower = i * bin_size\n",
    "#             bin_total = 0\n",
    "#             for j in range(lower,lower+bin_size):\n",
    "#                 bin_total += er[j]\n",
    "#             bins.append(bin_total)\n",
    "#         ind=numpy.arange(9)\n",
    "#         width=0.35\n",
    "#         bin_arr = numpy.array(bins)\n",
    "#         plt.bar(ind,bin_arr,width,color=\"blue\")\n",
    "#         display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "#         time.sleep(1.0)\n",
    "        \n",
    "#         writer = csv.writer(open('data.csv','wb'))\n",
    "#         writer.writerow([bin_arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final data file (\"fft_features_master.txt\") consists of 5,441 rows each consisting of 10 features corresponding to the frequency bins resulting from the above preprocessing. Each row ends with a class of either 1 (oil train) or 0 (not oil train).\n",
    "\n",
    "We rely on machine learning utilities developed for class and-or provided by the instructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget http://www.cs.colostate.edu/~anderson/cs480/notebooks/nn2.tar\n",
    "!mv nn2.tar.1 nn2.tar   # in case I already have a file named nn2.tar in this directory\n",
    "!tar xvf nn2.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from types import *\n",
    "import numpy as np\n",
    "import scaledconjugategradient as scg\n",
    "import mlutils as ml  # for draw()\n",
    "from copy import copy\n",
    "import sys  # for sys.float_info.epsilon\n",
    "import pdb\n",
    "import neuralnetworks as nn\n",
    "import mpl_toolkits.mplot3d as plt3\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import lda\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep = np.loadtxt('fft_features_master.txt',delimiter=',')\n",
    "T = prep[:,9:]\n",
    "X1 = prep[:,0:9]\n",
    "nrow,ncol = X1.shape\n",
    "nrows = X1.shape[0]\n",
    "nTrain = int(round(nrow*0.8))\n",
    "nTest = nrow - nTrain\n",
    "nTrain,nTest,nTrain+nTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = np.arange(nrows)\n",
    "np.random.shuffle(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainIndices = rows[:nTrain]\n",
    "testIndices = rows[nTrain:]\n",
    "trainIndices,testIndices\n",
    "Xtrain_fft = X1[trainIndices,:]\n",
    "Ttrain_fft = T[trainIndices,:]\n",
    "Xtest_fft = X1[testIndices,:]\n",
    "Ttest_fft = T[testIndices,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data ready, we establish some expectations with the most minimal neural network of 1 iteration and 0 hidden layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nHidden = 0\n",
    "nnet = nn.NeuralNetworkClassifier(9,nHidden,2) # 3 classes, will actually make 2-unit output layer\n",
    "nnet.train(Xtrain_fft,Ttrain_fft,  nIterations=1, verbose=True)\n",
    "%precision 3\n",
    "Ptrain,Prtrain,_ = nnet.use(Xtrain_fft,allOutputs=True)\n",
    "Ptest,Prtest,_ = nnet.use(Xtest_fft,allOutputs=True)\n",
    "print('Percent Correct: Training',100*np.sum(Ptrain==Ttrain_fft)/len(Ttrain_fft), 'Testing',100*np.sum(Ptest==Ttest_fft)/len(Ttest_fft))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single iteration with no hidden layers is essentially linear logistic regression, so a success rate of nearly 90 percent suggests that we may be able to achieve acceptable classification using a linear logistic regression-derived model. We reuse linear logistic regression functions developed for Assignment 4.\n",
    "\n",
    "We find that the singular value decomposition fails to converge for our training data. By removing the bias column, we are able to successfully return a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainLDA(X, T):\n",
    "    model = SimpleNamespace()\n",
    "    model.classes = np.unique(T)\n",
    "    model.means, model.stds = np.mean(X,0), np.std(X,0)\n",
    "    Xs = (X - model.means) / model.stds\n",
    "    model.mu = []\n",
    "    model.sigma = []\n",
    "    model.sigmaInv = []\n",
    "    model.determinant = []\n",
    "    model.prior = []\n",
    "    nSamples = X.shape[0]\n",
    "    for k in model.classes:\n",
    "        rowsThisClass = (T == k).reshape((-1))\n",
    "        #TODO make intersection of rtcl rtcm\n",
    "        model.mu.append( np.mean(Xs[rowsThisClass, :], 0).reshape((-1,1)) )\n",
    "        model.sigma.append( np.cov(Xs[rowsThisClass, :], rowvar=0) )\n",
    "        if model.sigma[-1].size == 1:\n",
    "            model.sigma[-1] = model.sigma[-1].reshape((1,1))\n",
    "        det = np.linalg.det(model.sigma[-1])\n",
    "        if det == 0:\n",
    "            det = sys.float_info.epsilon\n",
    "        model.determinant.append( det )\n",
    "        model.sigmaInv.append( np.linalg.pinv(model.sigma[-1]) )    # pinv in case Sigma is singular\n",
    "        model.prior.append( np.sum(rowsThisClass) / float(nSamples) )\n",
    "    model.sigmaMean = np.sum(np.stack(model.sigma) * np.array(model.prior)[:,np.newaxis,np.newaxis], axis=0)\n",
    "    model.sigmaMeanInv = np.linalg.pinv(model.sigmaMean)\n",
    "#     print(sigma)\n",
    "#     print(sigmaMean)\n",
    "    model.discriminantConstant = []\n",
    "    model.discriminantCoefficient = []\n",
    "    for ki in range(len(model.classes)):\n",
    "        model.sigmaMu = np.dot(model.sigmaMeanInv, model.mu[ki])\n",
    "        model.discriminantConstant.append( -0.5 * np.dot(model.mu[ki].T, model.sigmaMu) )\n",
    "        model.discriminantCoefficient.append( model.sigmaMu )\n",
    "    return model\n",
    "\n",
    "def evaluateLDA_detail(model, X, T):\n",
    "    def discriminantFunctionLDA(Xs,model):\n",
    "        nSamples = Xs.shape[0]\n",
    "        discriminants = np.zeros((nSamples, len(model.classes)))\n",
    "        for ki in range(len(model.classes)):\n",
    "            discriminants[:,ki:ki+1] = model.discriminantConstant[ki] + \\\n",
    "                                       np.dot(Xs, model.discriminantCoefficient[ki])\n",
    "        D = Xs.shape[1]\n",
    "        probabilities = np.exp( discriminants - 0.5*D*np.log(2*np.pi) - 0.5*np.log(model.determinant[ki]) \\\n",
    "                               - 0.5*np.sum(np.dot(Xs,model.sigmaMeanInv) * Xs, axis=1).reshape((-1,1)))\n",
    "        return discriminants, probabilities\n",
    "\n",
    "    def useLDA(model,X):\n",
    "        nSamples = X.shape[0]\n",
    "        Xs = (X - model.means) / model.stds\n",
    "        discriminants,probabilities = discriminantFunctionLDA(Xs,model)\n",
    "        predictedClass = model.classes[np.argmax( discriminants, axis=1 )]\n",
    "        predictedClass = predictedClass.reshape((-1, 1))\n",
    "        return predictedClass, probabilities, discriminants\n",
    "\n",
    "    cLDA, prob, d = useLDA(model, X)\n",
    "    accLDA = np.sum(cLDA == T) / X.shape[0] * 100\n",
    "    return accLDA, cLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xnobias = Xtrain_fft[:,1:9]\n",
    "model_LDA = trainLDA(Xnobias,Ttrain_fft)\n",
    "accLDA_cc, pred = evaluateLDA_detail(model_LDA, Xtest_fft[:,1:9], Ttest_fft)\n",
    "accLDA_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only linear logistic regression, we are able to achieve over 90 percent correctness. For our task, this is acceptable, but it seems likely that we can improve on this by using a somewhat less arbitrary feature set. This is described below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Set 2: Enhanced Spectral Contrast Features\n",
    "Spectral constrast corresponds to the decibal differences between the peaks and valleys among spectral bins. Smaller differences generally correspond to poorer audio comprehension, particularly in the hearing impaired. Audio features become \"smeared.\" Enhancing spectral constrast is analogous to enhancing image constrast. The differences between highs and lows are exaggerated. This is clearly desirable for our experiments as the entire goal is in isolating meaningful features from a highly noisy backdrop. \n",
    "\n",
    "Again, we provide only sample code for the feature extraction process as well as the resulting data file. Note that we are now using the Python audio analysis library Librosa for the feature extraction rather than SciPy's built in audio methods.\n",
    "\n",
    "With spectral contrast features now as our target, the feature extraction code listed above is ammended with the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "#         lower = i * frame_size\n",
    "#         upper = lower + frame_size - 1\n",
    "#         frames.append(signal[lower:upper])\n",
    "#     for frame in frames:\n",
    "#         stft = np.abs(librosa.stft(frame))\n",
    "#         contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "#         contrast_norm = np.around((contrast / np.linalg.norm(contrast)), decimals=3)\n",
    "#         if(rec[1] == 'oil'):\n",
    "#             oil = numpy.array(['1'])\n",
    "#             dat_out = numpy.hstack((contrast_norm,oil))\n",
    "#         else:\n",
    "#             non_oil = numpy.array(['0'])\n",
    "#             dat_out = numpy.hstack((contrast_norm, non_oil))\n",
    "\n",
    "#         # plt.bar(ind,bin_arr,width,color=\"blue\")\n",
    "#         # display.clear_output(wait=True)\n",
    "#         # display.display(plt.gcf())\n",
    "#         time.sleep(1.0)\n",
    "#         writer.writerow(['{:3}'.format(x) for x in dat_out])\n",
    "#         print dat_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature sets now consist of seven bins corresponding to ocatave-based frequency bands. Much of the programming for the second dataset is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep = np.loadtxt('sc_features_master.txt',delimiter=',')\n",
    "prep[:,0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = prep[:,7:]\n",
    "X = prep[:,0:7]\n",
    "X1 = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "nrow,ncol = X1.shape\n",
    "nrows = X1.shape[0]\n",
    "nTrain = int(round(nrow*0.8))\n",
    "nTest = nrow - nTrain\n",
    "nTrain,nTest,nTrain+nTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = np.arange(nrows)\n",
    "np.random.shuffle(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainIndices = rows[:nTrain]\n",
    "testIndices = rows[nTrain:]\n",
    "trainIndices,testIndices\n",
    "Xtrain = X1[trainIndices,:]\n",
    "Ttrain = T[trainIndices,:]\n",
    "Xtest = X1[testIndices,:]\n",
    "Ttest = T[testIndices,:]\n",
    "Xtrain.shape,Ttrain.shape, Xtest.shape,Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nHidden = 0\n",
    "nnet = nn.NeuralNetworkClassifier(8,nHidden,2) # 3 classes, will actually make 2-unit output layer\n",
    "nnet.train(Xtrain,Ttrain,  nIterations=1, verbose=True)\n",
    "%precision 3\n",
    "Ptrain,Prtrain,_ = nnet.use(Xtrain,allOutputs=True)\n",
    "Ptest,Prtest,_ = nnet.use(Xtest,allOutputs=True)\n",
    "print('Percent Correct: Training',100*np.sum(Ptrain==Ttrain)/len(Ttrain), 'Testing',100*np.sum(Ptest==Ttest)/len(Ttest))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the minimal neural network, we find a marginally acceptable accuracy, but one that is significantly lower than we found with the naive FFT bins. We'll now try with LDA, again with the bias column removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = trainLDA(Xtrain[:,1:7],Ttrain)\n",
    "accLDA_cc, pred = evaluateLDA_detail(model, Xtest[:,1:7], Ttest)\n",
    "print(str(accLDA_cc) + \"% correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, spectral contrast enhancement yields markedly more poor results. It seems most likely this has to do with the disconnect between raw industrial noise and music or speech. The Librosa spectral constrast features are based on musical octaves, while our samples are highly atonal. We take these results as somewhat of a validation of our initial hypothesis that our classification would succeed with a naive approach to using acoustic frequencies as features. \n",
    "\n",
    "It may also be the case that using the Short-Time Fourier Transform to prepare this feature set rather than the FFT has limited some resolution. For further evaluation, we rely on the first feature set (naive FFT).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Further Model Evaluation \n",
    "First, we determine a reasonable neural network configuration that maximizes accuracy without adding too much complexity. We test seven configurations below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%precision 3\n",
    "params = [(15,15),(5,15),(10,5),(2,3),(1,5),(5,5),(1,1)]\n",
    "\n",
    "for param in params:\n",
    "    nnet = nn.NeuralNetworkClassifier(9,param[0],2) # 3 classes, will actually make 2-unit output layer\n",
    "    nnet.train(Xtrain_fft,Ttrain_fft,  nIterations=param[1], verbose=True)\n",
    "    Ptrain,Prtrain,_ = nnet.use(Xtrain_fft,allOutputs=True)\n",
    "    Ptest,Prtest,_ = nnet.use(Xtest_fft,allOutputs=True)\n",
    "    acc_train = 100*np.sum(Ptrain==Ttrain_fft)/len(Ttrain_fft)\n",
    "    acc_test = 100*np.sum(Ptrain==Ttrain_fft)/len(Ttrain_fft)\n",
    "    print(\"accuracy train on \"+ str(param[0]) + \" layers \"  + str(param[1]) + \" iterations \" + str(acc_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to gain very little by adding layers, bouying our prior suggestion that a neural network-based model offers very little for the additional complexity given our ideal low-resource environment (embedded, IoT, etc). Moving forward, we will based further evaluations on LDA models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of our model is the classification of railcars in the real-world. So far, we have experimented with a highly idealized scenario wherin recordings have been collected from a single location in similar conditions. For a system designed to operate in a variety of conditions&mdash;varying distances from trains, unpredictable background noise, weather&mdash;this is insufficient. Here, we evaluate our model's performance on new data consisting of progressively degraded recordings of oil cars. \n",
    "\n",
    "We add noise to existing FFT bins using a Gaussian distribution with mean 0 and an initial standard deviation of .1: <pre>noise = np.random.normal(0,.1,signal.size)</pre> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_sample = np.loadtxt(\"noise_samples.txt\",delimiter=\",\")\n",
    "Tnoisy = noise_sample[:,9:]\n",
    "Xnoisy = noise_sample[:,1:9]\n",
    "noise_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(Tnoisy.size):\n",
    "    noise = np.random.normal(0,.1,8)\n",
    "    Xnoisy[i] = Xnoisy[i] + noise\n",
    "test_acc, test_pred = evaluateLDA_detail(model_LDA,Xnoisy,Tnoisy)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a noise factor generated with standard deviation of .1, accuracy declines to about 63 percent. Given that .1 represents distortions of between 0 and 10 percent of the intact signal (itself normalized from 0 to 1), that indicates the model (previously trained) is at least somewhat robust. Let's try some other values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_acc = []\n",
    "def noise_run(Xx, noise_sd,size):\n",
    "    for i in range(size):\n",
    "        noise = np.random.normal(0,noise_sd,8)\n",
    "        Xx[i] = Xx[i] + noise\n",
    "    return Xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noises = [0,.01,.05,.075,.1,.15,.2,.3]\n",
    "noise_acc = []\n",
    "for noise in noises:\n",
    "    acc,pred = evaluateLDA_detail(model_LDA, noise_run(Xnoisy,noise,Tnoisy.size),Tnoisy)\n",
    "    noise_acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(noises,noise_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a smooth drop-off as more distortion is added to the fft bins, all the way up to .3, which represents nearly a third of the total normalized bins. It's impressive that the signal can be so degraded and the LDA model returns more than 50 percent accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Work\n",
    "Our work here is limited in several respects. Though we programmatically add distortion, this still doesn't represent the effects of real-world environmental variance likely to be faced by a production system. Future work will replace section 6 with recordings taken in different locations at different distances. Similarly, we will need to demonstrate the robustness of the model in differentiating oil cars from all other possible railcars. Limiting ourselves to intermodal cars is a significant limitation of this work. This was, however, a neccessary limitation given available data sizes. Many more recordings will need to be processed and accumulated. It remains to be determined whether or not a new model will be required for every railcar variety, or if our current approach will continue to suffice (oil cars vs. \"other\").\n",
    "\n",
    "Finally, it's our ultimate goal for this system to operate within an embedded trackside system doing classifications in real-time. We imagine, moreover, such a system to be capable of updating its own models to better fit its current environment. Style transfer[7], a recent advance in image recognition/generation, suggests that it may indeed be possible to have a system capable of updating a pretrained model. The suitability of linear models for this task is promising. A next step will be prototyping a device and testing it in the same location that the data for this report was collected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "We have demonstrated the possibility of using linear models for the classification of non-time series environmental audio. With accuracy rates of between 70 and 90 percent for an idealized case using data collected in one location our system is already suited for limited acoustic classification tasks. Moreover, using artificially degraded data we found that the model remains robust even as frequency-domain features are distorted as much as 30 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "[1] Omar et al, \"Machine Learning Techniques for Anomaly Detection:\n",
    "An Overview\", https://pdfs.semanticscholar.org/0278/bbaf1db5df036f02393679d485260b1daeb7.pdf\n",
    "[2] Aheard, \"Growing oil train traffic is shrouded in secrecy\", https://www.revealnews.org/article/growing-oil-train-traffic-is-shrouded-in-secrecy/\n",
    "[3] \"Urban Sound Classification\", https://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/\n",
    "[4] Shamir, \"Classification of large acoustic datasets using machine learning and crowdsourcing: Application to whale calls\", http://asa.scitation.org/doi/pdf/10.1121/1.4861348\n",
    "[5] Yen, \"AUTOMATIC FROG CALLS MONITORING SYSTEM: A MACHINE LEARNING APPROACH\", DOI: 10.1142/S1469026801000184\n",
    "[6] Acevedo, \"Automated classification of bird and amphibian calls using machine learning: A comparison of methods\", Automated classification of bird and amphibian calls using machine learning: A comparison of methods\n",
    "[7] \"Image Style Transfer Using Convolutional Neural Networks\", http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
